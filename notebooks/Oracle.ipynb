{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle Jaycar Part detection proof of concept\n",
    "\n",
    "This is a part detector for use within jaycar stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base directories and variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_dir = '/tmp/oracle'\n",
    "image_bucket = 'oracle-reference-images'\n",
    "\n",
    "%mkdir /tmp/oracle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "these are jpg images, Sagemaker built in expects dataset formatted in Record IO so we need to prepare the RECORDIO files\n",
    "these images are in our `~/oracle/images/AZ0--9` directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-mnist in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.6)\n",
      "downloading PA3566/1539557366_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557366_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557367_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557376_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557376_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557377_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557381_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557381_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557382_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557385_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557385_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557385_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557391_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557391_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557392_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557394_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557395_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557395_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557399_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557399_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557400_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557402_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557403_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557403_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557405_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557406_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557406_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557408_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557408_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557409_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557411_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557411_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557412_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557414_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557414_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557415_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557416_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557417_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557417_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557419_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557419_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557420_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557422_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557422_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557423_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557429_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557430_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557430_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557432_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557432_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557433_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557435_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557436_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557436_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557439_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557439_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557440_2.jpg into images/PA3566\n",
      "downloading PA3566/1539557443_0.jpg into images/PA3566\n",
      "downloading PA3566/1539557443_1.jpg into images/PA3566\n",
      "downloading PA3566/1539557444_2.jpg into images/PA3566\n",
      "downloading PP0800/1539559018_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559018_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559019_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559022_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559022_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559023_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559025_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559025_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559026_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559027_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559028_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559028_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559030_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559031_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559031_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559033_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559034_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559034_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559036_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559037_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559037_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559039_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559040_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559040_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559042_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559042_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559043_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559045_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559045_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559046_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559047_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559048_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559048_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559050_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559050_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559051_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559053_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559053_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559054_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559056_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559057_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559057_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559060_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559061_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559061_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559064_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559064_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559064_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559067_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559067_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559068_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559070_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559071_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559071_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559074_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559075_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559075_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559078_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559078_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559079_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559083_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559083_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559084_2.jpg into images/PP0800\n",
      "downloading PP0800/1539559087_0.jpg into images/PP0800\n",
      "downloading PP0800/1539559087_1.jpg into images/PP0800\n",
      "downloading PP0800/1539559088_2.jpg into images/PP0800\n",
      "downloading PS0804/1539559121_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559121_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559122_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559124_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559124_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559125_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559127_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559127_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559128_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559130_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559130_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559130_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559133_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559133_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559134_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559136_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559136_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559136_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559141_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559141_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559142_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559144_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559144_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559144_2.jpg into images/PS0804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading PS0804/1539559146_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559147_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559147_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559149_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559149_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559150_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559154_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559155_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559155_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559159_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559160_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559160_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559163_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559163_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559163_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559165_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559166_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559166_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559169_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559169_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559170_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559172_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559172_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559173_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559176_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559177_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559177_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559182_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559182_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559183_2.jpg into images/PS0804\n",
      "downloading PS0804/1539559203_0.jpg into images/PS0804\n",
      "downloading PS0804/1539559203_1.jpg into images/PS0804\n",
      "downloading PS0804/1539559204_2.jpg into images/PS0804\n",
      "downloading PT3020/1539558488_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558488_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558489_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558495_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558495_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558496_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558498_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558499_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558499_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558503_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558503_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558504_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558507_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558508_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558508_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558511_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558511_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558512_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558514_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558515_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558515_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558517_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558517_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558518_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558521_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558521_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558522_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558524_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558525_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558525_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558528_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558528_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558529_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558533_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558533_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558534_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558537_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558537_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558538_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558540_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558541_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558541_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558546_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558546_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558547_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558557_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558557_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558558_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558561_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558562_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558562_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558568_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558568_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558569_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558573_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558574_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558574_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558581_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558582_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558582_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558586_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558587_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558587_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558591_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558592_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558592_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558595_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558595_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558596_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558598_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558599_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558599_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558602_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558602_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558603_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558605_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558606_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558606_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558669_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558669_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558670_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558672_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558672_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558673_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558681_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558682_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558682_2.jpg into images/PT3020\n",
      "downloading PT3020/1539558764_0.jpg into images/PT3020\n",
      "downloading PT3020/1539558765_1.jpg into images/PT3020\n",
      "downloading PT3020/1539558765_2.jpg into images/PT3020\n"
     ]
    }
   ],
   "source": [
    "!pip install python-mnist\n",
    "import os\n",
    "import boto3\n",
    "import io\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "\n",
    "#get role \n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "os.chdir(base_dir)\n",
    "os.mkdir('images')\n",
    "objlist = s3.list_objects(Bucket=image_bucket)\n",
    "files = []\n",
    "\n",
    "for c in objlist['Contents']:\n",
    "    k = c['Key']\n",
    "    d = 'images/' + k.split('/')[0]\n",
    "    print('downloading {} into {}'.format(k,d))\n",
    "    try:\n",
    "        os.mkdir(d)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    s3.download_file(image_bucket, k,'images/'+k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n",
      "env: IM2REC=/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/tools/im2rec.py\n",
      "env: BASE_DIR=/tmp/oracle\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "\n",
    "suffix='/mxnet/tools/im2rec.py'\n",
    "im2rec = list(filter( (lambda x: os.path.isfile(x + suffix )), sys.path))[0] + suffix\n",
    "print(im2rec)\n",
    "%env IM2REC=$im2rec\n",
    "%env BASE_DIR=$base_dir\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the databse into test and train\n",
    "this uses IM2REC to make different listings of what files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA3566 0\n",
      "PP0800 1\n",
      "PS0804 2\n",
      "PT3020 3\n",
      "oracle_test.lst\n",
      "oracle_train.lst\n",
      "==> oracle_test.lst <==\n",
      "143\t2.000000\tPS0804/1539559136_2.jpg\n",
      "16\t0.000000\tPA3566/1539557395_1.jpg\n",
      "173\t2.000000\tPS0804/1539559173_2.jpg\n",
      "187\t3.000000\tPT3020/1539558495_1.jpg\n",
      "5\t0.000000\tPA3566/1539557377_2.jpg\n",
      "72\t1.000000\tPP0800/1539559030_0.jpg\n",
      "223\t3.000000\tPT3020/1539558541_1.jpg\n",
      "81\t1.000000\tPP0800/1539559039_0.jpg\n",
      "97\t1.000000\tPP0800/1539559053_1.jpg\n",
      "191\t3.000000\tPT3020/1539558499_2.jpg\n",
      "\n",
      "==> oracle_train.lst <==\n",
      "98\t1.000000\tPP0800/1539559054_2.jpg\n",
      "120\t1.000000\tPP0800/1539559083_0.jpg\n",
      "172\t2.000000\tPS0804/1539559172_1.jpg\n",
      "71\t1.000000\tPP0800/1539559028_2.jpg\n",
      "153\t2.000000\tPS0804/1539559149_0.jpg\n",
      "238\t3.000000\tPT3020/1539558574_1.jpg\n",
      "237\t3.000000\tPT3020/1539558573_0.jpg\n",
      "270\t3.000000\tPT3020/1539558764_0.jpg\n",
      "0\t0.000000\tPA3566/1539557366_0.jpg\n",
      "157\t2.000000\tPS0804/1539559155_1.jpg\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd $BASE_DIR\n",
    "python $IM2REC --list --recursive --test-ratio=0.3 --train-ratio=0.7 oracle images\n",
    "ls *.lst\n",
    "head *.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .rec file from /tmp/oracle/oracle_test.lst in /tmp/oracle\n",
      "time: 0.0007343292236328125  count: 0\n",
      "Creating .rec file from /tmp/oracle/oracle_train.lst in /tmp/oracle\n",
      "time: 0.0007202625274658203  count: 0\n",
      "oracle_test.rec\n",
      "oracle_train.rec\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "\n",
    "cd $BASE_DIR\n",
    "python $IM2REC --num-thread=4 --pass-through oracle_test.lst images\n",
    "python $IM2REC --num-thread=4 --pass-through oracle_train.lst images\n",
    "ls *.rec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload rec files to S3 for safekeeping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-ap-southeast-2-349967396867\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "#INFO:sagemaker:Created S3 bucket: sagemaker-ap-southeast-2-349967396867\n",
    "\n",
    "train_path = sagemaker_session.upload_data(path=base_dir+'/oracle_train.rec',key_prefix='oracle/train')\n",
    "train_path = sagemaker_session.upload_data(path=base_dir+'/oracle_test.rec',key_prefix='oracle/test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup your Environment\n",
    "Permissions and environment variables\n",
    "\n",
    "Here we set up the linkage and authentication to AWS services. There are three parts to this:\n",
    "\n",
    "1. The roles used to give learning and hosting access to your data. This will automatically be obtained from the role used to start the notebook\n",
    "2. The S3 bucket that you want to use for training and model data\n",
    "3. The Amazon sagemaker image classification docker image which need not be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544295431143.dkr.ecr.ap-southeast-2.amazonaws.com/image-classification:latest\n",
      "CPU times: user 41.1 ms, sys: 3.58 ms, total: 44.7 ms\n",
      "Wall time: 858 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# 1. Obtaining the role you already configured for Sagemaker when you setup\n",
    "# your Instance notebook (https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)\n",
    "role = get_execution_role()\n",
    "\n",
    "# 2. The S3 Bucket that will store the dataset and the trained model\n",
    "# It was already defined above, while we uploaded the RecordIO files to the S3 bucket.\n",
    "\n",
    "# 3. Select the correct Docker image with the Image Classification algorithm\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/image-classification:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/image-classification:latest',\n",
    "              'ap-southeast-2': '544295431143.dkr.ecr.ap-southeast-2.amazonaws.com/image-classification:latest'}\n",
    "training_image = containers[boto3.Session().region_name]\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm supports multiple network depth (number of layers). They are 18, 34, 50, 101, 152 and 200\n",
    "# For this training, we will use 152 layers\n",
    "num_layers = 152\n",
    "\n",
    "# we need to specify the input image shape for the training data\n",
    "image_shape = \"3,240,480\"\n",
    "\n",
    "# we also need to specify the number of training samples in the training set\n",
    "num_training_samples = 273\n",
    "\n",
    "# specify the number of output classes\n",
    "num_classes = 4\n",
    "\n",
    "# batch size for training\n",
    "mini_batch_size = 5\n",
    "\n",
    "# number of epochs\n",
    "epochs = 40\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.00001\n",
    "\n",
    "# Since we are using transfer learning, we set use_pretrained_model to 1 so that weights can be \n",
    "# initialized with pre-trained weights\n",
    "use_pretrained_model = 1\n",
    "\n",
    "# Training algorithm/optimizer. Default is SGD\n",
    "optimizer = 'sgd'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Job\n",
    "using below params and hyper params. send a job off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: oracle-2018-10-15-04-35-29\n",
      "\n",
      "Input Data Location: {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-ap-southeast-2-349967396867/oracle/train/', 'S3DataDistributionType': 'FullyReplicated'}\n"
     ]
    }
   ],
   "source": [
    "dataset_prefix='oracle'\n",
    "# create unique job name \n",
    "job_name_prefix = 'oracle'\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "job_name = job_name_prefix + timestamp\n",
    "\n",
    "training_params = {}\n",
    "\n",
    "# Here we set the reference for the Image Classification Docker image, stored on ECR (https://aws.amazon.com/pt/ecr/)\n",
    "training_params[\"AlgorithmSpecification\"] = {\n",
    "    \"TrainingImage\": training_image,\n",
    "    \"TrainingInputMode\": \"File\"\n",
    "}\n",
    "\n",
    "# The IAM role with all the permissions given to Sagemaker\n",
    "training_params[\"RoleArn\"] = role\n",
    "\n",
    "# Here Sagemaker will store the final trained model\n",
    "training_params[\"OutputDataConfig\"] = {\n",
    "    \"S3OutputPath\": 's3://{}/{}/output'.format(default_bucket, job_name_prefix)\n",
    "}\n",
    "\n",
    "# This is the config of the instance that will execute the training\n",
    "training_params[\"ResourceConfig\"] = {\n",
    "    \"InstanceCount\": 1,\n",
    "    \"InstanceType\": \"ml.p2.xlarge\",\n",
    "    \"VolumeSizeInGB\": 50\n",
    "}\n",
    "\n",
    "# The job name. You'll see this name in the Jobs section of the Sagemaker's console\n",
    "training_params[\"TrainingJobName\"] = job_name\n",
    "\n",
    "# Here you will configure the hyperparameters used for training your model.\n",
    "training_params[\"HyperParameters\"] = {\n",
    "    \"image_shape\": image_shape,\n",
    "    \"num_layers\": str(num_layers),\n",
    "    \"num_training_samples\": str(num_training_samples),\n",
    "    \"num_classes\": str(num_classes),\n",
    "    \"mini_batch_size\": str(mini_batch_size),\n",
    "    \"epochs\": str(epochs),\n",
    "    \"learning_rate\": str(learning_rate),\n",
    "    \"use_pretrained_model\": str(use_pretrained_model),\n",
    "    \"optimizer\": optimizer\n",
    "}\n",
    "\n",
    "# Training timeout\n",
    "training_params[\"StoppingCondition\"] = {\n",
    "    \"MaxRuntimeInSeconds\": 360000\n",
    "}\n",
    "\n",
    "# The algorithm currently only supports fullyreplicated model (where data is copied onto each machine)\n",
    "training_params[\"InputDataConfig\"] = []\n",
    "\n",
    "# Please notice that we're using application/x-recordio for both \n",
    "# training and validation datasets, given our dataset is formated in RecordIO\n",
    "\n",
    "# Here we set training dataset\n",
    "# Training data should be inside a subdirectory called \"train\"\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"train\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/train/'.format(default_bucket, dataset_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "# Here we set validation dataset\n",
    "# Validation data should be inside a subdirectory called \"validation\"\n",
    "training_params[\"InputDataConfig\"].append({\n",
    "    \"ChannelName\": \"validation\",\n",
    "    \"DataSource\": {\n",
    "        \"S3DataSource\": {\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3Uri\": 's3://{}/{}/test/'.format(default_bucket, dataset_prefix),\n",
    "            \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "        }\n",
    "    },\n",
    "    \"ContentType\": \"application/x-recordio\",\n",
    "    \"CompressionType\": \"None\"\n",
    "})\n",
    "\n",
    "print('Training job name: {}'.format(job_name))\n",
    "print('\\nInput Data Location: {}'.format(training_params['InputDataConfig'][0]['DataSource']['S3DataSource']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "You'll create your model in four steps:\n",
    "  1. First you'll submit a job for sagemaker to train your model using your dataset uploaded to S3\n",
    "  2. Second you'll pack the job output into model ready to be used\n",
    "  3. Then you'll create an Endpoint Configuration, which is the metadata used by Sagemaker to deploy your model\n",
    "  4. Finally you'll deploy your model using the Endpoint Configuration\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job current status: InProgress\n",
      "Training job ended with status: Completed\n"
     ]
    }
   ],
   "source": [
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n",
    "print('Training job current status: {}'.format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter('training_job_completed_or_stopped').wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info['TrainingJobStatus']\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print('Training failed to start')\n",
    "     # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)['FailureReason']\n",
    "    print('Training failed with the following error: {}'.format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oracle-2018-10-15-05-11-57\n",
      "s3://sagemaker-ap-southeast-2-349967396867/oracle/output/oracle-2018-10-15-04-35-29/output/model.tar.gz\n",
      "arn:aws:sagemaker:ap-southeast-2:349967396867:model/oracle-2018-10-15-05-11-57\n",
      "CPU times: user 14.9 ms, sys: 70 µs, total: 14.9 ms\n",
      "Wall time: 481 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from time import gmtime, strftime\n",
    "\n",
    "use_pretrained_model=False\n",
    "bucket = default_bucket\n",
    "\n",
    "model_name=\"oracle\" + time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "print(model_name)\n",
    "\n",
    "info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info['ModelArtifacts']['S3ModelArtifacts']\n",
    "print(model_data)\n",
    "\n",
    "primary_container = {\n",
    "    'Image': training_image,\n",
    "    'ModelDataUrl': model_data,\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_model_response = sagemaker.create_model(\n",
    "        ModelName = model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        PrimaryContainer = primary_container)\n",
    "    print(create_model_response['ModelArn'])\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create endpoint config for model.\n",
    "support configuring rest endpoints in hosting with multiple models eg for A/B testing purposes. customers create an endpoint configuration, that describes the distrution of traffic across the models, either split shadowed or sampled in some way.\n",
    "\n",
    "hence the:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint configuration name: oracle-epc-2018-10-15-05-13-45\n",
      "Endpoint configuration arn:  arn:aws:sagemaker:ap-southeast-2:349967396867:endpoint-config/oracle-epc-2018-10-15-05-13-45\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + '-epc-' + timestamp\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.c4.2xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: oracle-ep-2018-10-15-05-14-16\n",
      "EndpointArn = arn:aws:sagemaker:ap-southeast-2:349967396867:endpoint/oracle-ep-2018-10-15-05-14-16\n",
      "EndpointStatus = Creating\n",
      "Endpoint creation ended with EndpointStatus = InService\n",
      "CPU times: user 161 ms, sys: 0 ns, total: 161 ms\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "timestamp = time.strftime('%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "endpoint_name = job_name_prefix + '-ep-' + timestamp\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))\n",
    "\n",
    "\n",
    "# get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test your model\n",
    "Let's recapitulate. We've just trained an Image Classifier model using `data`. What does that mean? It means that now, we have an 'WebService' accessible by the endpoint we just deployed, that has the power to classify 4 different types of objects\n",
    "\n",
    "lets try:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "%matplotlib inline\n",
    "\n",
    "runtime = boto3.Session().client(service_name='sagemaker-runtime') \n",
    "object_categories = ['PT3020', 'PS0804','PP0800','PA3566']\n",
    "\n",
    "_, axarr = plt.subplots(1, 4, figsize=(20,12))\n",
    "col = 0\n",
    "for i in range(4):\n",
    "    \n",
    "    # Load the image bytes\n",
    "    img = open(base_dir+'{}.jpg'.format(i)).read()\n",
    "    \n",
    "    # Call your model for predicting which object appears in this image.\n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        ContentType='application/x-image', \n",
    "        Body=bytearray(img)\n",
    "    )\n",
    "    # read the prediction result and parse the json\n",
    "    result = response['Body'].read()\n",
    "    result = json.loads(result)\n",
    "    \n",
    "    # which category has the highest confidence?\n",
    "    pred_label_id = np.argmax(result)\n",
    "    \n",
    "    # Green when our model predicted correctly, otherwise, Red\n",
    "    text_color = 'red'\n",
    "    if object_categories[pred_label_id] == test_categories[i]:\n",
    "        text_color = 'green'\n",
    "\n",
    "    # Render the text for each image/prediction\n",
    "    output_text = '%s (%f)' %(object_categories[pred_label_id], result[pred_label_id] )\n",
    "    axarr[col].text(0, 0, output_text, fontsize=15, color=text_color)\n",
    "    print( output_text )\n",
    "    \n",
    "    # Render the image\n",
    "    img = Image.open(BytesIO(img))\n",
    "    frame = axarr[col].imshow(img)\n",
    "    \n",
    "    col += 1\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
